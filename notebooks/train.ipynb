{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to train RNN model with/without attention for transliteration task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import regex as re\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import wandb\n",
    "import lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.nn.functional import pad\n",
    "import gc\n",
    "from ..src.models import RNN_light, RNN_light_attention\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ..src.dataloader import NativeTokenizer, LatNatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_API_KEY'] = \"key\"\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\"\n",
    "valid_path = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\"\n",
    "test_path = \"/kaggle/input/dakshina/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None, names=[\"native\", \"latin\", 'n_annot'], encoding='utf-8')\n",
    "valid_df = pd.read_csv(valid_path, sep=\"\\t\", header=None, names=[\"native\", \"latin\", 'n_annot'], encoding='utf-8')\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\", header=None, names=[\"native\", \"latin\", 'n_annot'], encoding='utf-8')\n",
    "\n",
    "train_df = train_df[~train_df['latin'].isna()]\n",
    "valid_df = valid_df[~valid_df['latin'].isna()]\n",
    "test_df = test_df[~test_df['latin'].isna()]\n",
    "\n",
    "\n",
    "tokenizer = NativeTokenizer(train_path, valid_path, test_path)\n",
    "print(f\"Latin vocab size: {tokenizer.latin_vocab_size}\")\n",
    "print(f\"Native vocab size: {tokenizer.nat_vocab_size}\")\n",
    "\n",
    "train_dataset = LatNatDataset(train_df, tokenizer)\n",
    "valid_dataset = LatNatDataset(valid_df, tokenizer)\n",
    "test_dataset = LatNatDataset(test_df, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn, num_workers=2)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=valid_dataset.collate_fn , num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_dataset.collate_fn, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {key: val for key, val in tokenizer.native_vocab.items() if key in ['<start>', '<end>', '<pad>']}\n",
    "\n",
    "model = RNN_light_attention(\n",
    "            input_sizes=(tokenizer.latin_vocab_size, tokenizer.nat_vocab_size),\n",
    "            embedding_size=128,\n",
    "            hidden_size=256,\n",
    "            cell='LSTM',\n",
    "            layers=3,\n",
    "            dropout=0.25,\n",
    "            activation='tanh',\n",
    "            beam_size=3,\n",
    "            optim='adam',\n",
    "            special_tokens=special_tokens,\n",
    "            lr=0.0002)\n",
    "\n",
    "logger= WandbLogger(project= 'DLA3_sweeps', name = \"bestmodel\") #,resume=\"never\")\n",
    "trainer = pl.Trainer(max_epochs=1,  accelerator=\"auto\",logger=logger, profiler='simple',  precision=\"16-mixed\",)\n",
    "trainer.fit(model, train_dataloader,  valid_dataloader)\n",
    "trainer.test(model, dataloaders=test_dataloader)\n",
    " \n",
    "rand_ind = np.random.choice(len(model.test_preds), size=9, replace=False)\n",
    "attention_map = [model.attention_maps[ind] for ind in rand_ind]\n",
    "src = np.array(model.test_inputs)[rand_ind]\n",
    "pred = np.array(model.test_preds)[rand_ind]\n",
    "tgt = np.array(model.test_labels)[rand_ind]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle(\"Attention map\", fontsize=16)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    attn_map = attention_map[i][0:len(pred[i]), 0:len(src[i])]  \n",
    "\n",
    "    sns.heatmap(attn_map, ax=ax, xticklabels=src[i], yticklabels=pred[i],\n",
    "                cmap=\"Blues\", cbar=True)\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=8)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    fig.supxlabel(\"Latin script\", fontsize=14)\n",
    "    fig.supylabel(\"Tamil script\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_light(\n",
    "            input_sizes=(tokenizer.latin_vocab_size, tokenizer.nat_vocab_size),\n",
    "            embedding_size=128,\n",
    "            hidden_size=256,\n",
    "            cell='LSTM',\n",
    "            layers=3,\n",
    "            dropout=0.25,\n",
    "            activation='tanh',\n",
    "            beam_size=3,\n",
    "            optim='adam',\n",
    "            special_tokens=special_tokens,\n",
    "            lr=0.0002)\n",
    "\n",
    "logger= WandbLogger(project= 'DLA3_sweeps', name = \"bestmodel\") #,resume=\"never\")\n",
    "trainer = pl.Trainer(max_epochs=1,  accelerator=\"auto\",logger=logger, profiler='simple',  precision=\"16-mixed\",)\n",
    "trainer.fit(model, train_dataloader,  valid_dataloader)\n",
    "trainer.test(model, dataloaders=test_dataloader)\n",
    "\n",
    "rand_ind = np.random.choice(len(model.test_preds), size=9, replace=False)\n",
    "src = np.array(model.test_inputs)[rand_ind]\n",
    "tgt = np.array(model.test_labels)[rand_ind]\n",
    "preds = np.array(model.test_preds)[rand_ind]\n",
    "\n",
    "# table fo comparison\n",
    "fig, ax = plt.subplots(figsize=(10, len(src) * 1.5))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "table_data = [[\"Input\", \"Actual\", \"Prediction\"]]\n",
    "for inp, true, pred in zip(src, tgt, preds):\n",
    "    table_data.append([inp, true, pred])\n",
    "\n",
    "table = ax.table(cellText=table_data, colLabels=None, loc='center', cellLoc='left')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1, 1.5)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
