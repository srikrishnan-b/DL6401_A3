{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import regex as re\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native</th>\n",
       "      <th>latin</th>\n",
       "      <th>n_annot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ஃபியட்</td>\n",
       "      <td>fiat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ஃபியட்</td>\n",
       "      <td>phiyat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ஃபியட்</td>\n",
       "      <td>piyat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ஃபிரான்ஸ்</td>\n",
       "      <td>firaans</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ஃபிரான்ஸ்</td>\n",
       "      <td>france</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      native    latin  n_annot\n",
       "0     ஃபியட்     fiat        2\n",
       "1     ஃபியட்   phiyat        1\n",
       "2     ஃபியட்    piyat        1\n",
       "3  ஃபிரான்ஸ்  firaans        1\n",
       "4  ஃபிரான்ஸ்   france        2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = \"/home/user/Documents/Courses/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\"\n",
    "valid_path = \"/home/user/Documents/Courses/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\"\n",
    "test_path = \"/home/user/Documents/Courses/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None, names=[\"native\", \"latin\", 'n_annot'], encoding='utf-8')\n",
    "valid_df = pd.read_csv(valid_path, sep=\"\\t\", header=None, names=[\"native\", \"latin\", 'n_annot'], encoding='utf-8')\n",
    "test_df = pd.read_csv(test_path, sep=\"\\t\", header=None, names=[\"native\", \"latin\", 'n_annot'], encoding='utf-8')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NativeTokenizer():\n",
    "    def __init__(self, train_path, valid_path, test_path, special_tokens={'START': '<start>','END':'<end>', 'PAD':'<pad>'}):\n",
    "        \n",
    "        self.train_df = pd.read_csv(train_path, sep=\"\\t\", header=None, names=[\"native\", \"latin\", 'n_annot'], encoding='utf-8')\n",
    "        self.valid_df = pd.read_csv(valid_path, sep=\"\\t\", header=None, names=[\"native\", \"latin\", 'n_annot'], encoding='utf-8')\n",
    "        self.test_df = pd.read_csv(test_path, sep=\"\\t\", header=None, names=[\"native\", \"latin\", 'n_annot'], encoding='utf-8')\n",
    "        self.special_tokens = special_tokens\n",
    "        # Build vocabulary\n",
    "        self._build_vocab(add_special_tokens=True)\n",
    "        \n",
    "        # Id to token mapping\n",
    "        self.id_to_latin = {i: char for i, char in enumerate(self.latin_vocab)}\n",
    "        self.id_to_native = {i: char for i, char in enumerate(self.native_vocab)}\n",
    "\n",
    "        self.latin_vocab_size = len(self.latin_vocab)\n",
    "        self.nat_vocab_size = len(self.native_vocab)\n",
    "\n",
    "    # Build vocabulary\n",
    "    def _build_vocab(self, add_special_tokens=True):\n",
    "        self.nat_set = set()\n",
    "        self.latin_set = set()\n",
    "        for lat, nat in zip(self.train_df['latin'], self.train_df['native']):\n",
    "            nat_chars = re.findall(r'\\X' , nat)\n",
    "            try:\n",
    "                lat_chars = list(lat)\n",
    "            except:\n",
    "                print(f\"Invalid latin string: {lat}, skipping....\")\n",
    "            \n",
    "            for char in nat_chars:\n",
    "                self.nat_set.add(char)\n",
    "            for char in lat_chars:\n",
    "               self.latin_set.add(char.lower())\n",
    "            \n",
    "        self.nat_set = sorted(list(self.nat_set))\n",
    "        self.latin_set = sorted(list(self.latin_set))\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            self.nat_set = list(self.special_tokens.values()) + self.nat_set\n",
    "            self.latin_set = [self.special_tokens['PAD']] + self.latin_set   \n",
    "\n",
    "        self.latin_vocab = {char: i for i, char in enumerate(self.latin_set)}\n",
    "        self.native_vocab = {char: i for i, char in enumerate(self.nat_set)}\n",
    "\n",
    "    def tokenize(self, text, lang='latin'):\n",
    "        if lang == 'latin':\n",
    "            return [self.latin_vocab[char] for char in text]\n",
    "        elif lang == 'native':\n",
    "            return [self.native_vocab['<start>']] + [self.native_vocab[char] for char in re.findall('\\X', text)] + [self.native_vocab['<end>']]\n",
    "        else:\n",
    "            raise ValueError(\"Language must be either 'latin' or 'native'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid latin string: nan, skipping....\n",
      "Invalid latin string: nan, skipping....\n",
      "Invalid latin string: nan, skipping....\n",
      "Latin vocab size: 27\n",
      "Native vocab size: 253\n"
     ]
    }
   ],
   "source": [
    "tokenizer = NativeTokenizer(train_path, valid_path, test_path)\n",
    "print(f\"Latin vocab size: {tokenizer.latin_vocab_size}\")\n",
    "print(f\"Native vocab size: {tokenizer.nat_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatNatDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.df.iloc[idx]\n",
    "        latin_word = entry['latin']\n",
    "        native_word = entry['native']\n",
    "               \n",
    "        # Tokenize and convert to IDs\n",
    "        #latin_ids = [self.tokenizer.latin_vocab[i] for i in latin_word]\n",
    "        #native_ids = [self.tokenizer.native_vocab[i] for i in re.findall(r'\\X' , native_word)]\n",
    "        latin_ids = self.tokenizer.tokenize(latin_word, lang='latin')\n",
    "        native_ids = self.tokenizer.tokenize(native_word, lang='native')\n",
    "\n",
    "\n",
    "        return (torch.tensor(latin_ids),\n",
    "            torch.tensor(native_ids))\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        x,y = zip(*batch)\n",
    "        x_len = [len(seq) for seq in x]\n",
    "        y_len = [len(seq) for seq in y]\n",
    "\n",
    "        padded_x = pad_sequence(x, batch_first=True, padding_value=self.tokenizer.latin_vocab['<pad>'])\n",
    "        padded_y = pad_sequence(y, batch_first=True, padding_value=self.tokenizer.native_vocab['<pad>'])\n",
    "        \n",
    "        x_len, perm_idx = torch.tensor(x_len).sort(0, descending=True)\n",
    "        padded_x = padded_x[perm_idx]\n",
    "\n",
    "        y_len = torch.tensor(y_len).sort(0, descending=True)\n",
    "        padded_y = padded_y[perm_idx]\n",
    "\n",
    "        return padded_x, x_len, padded_y, y_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LatNatDataset(train_df, tokenizer)\n",
    "valid_dataset = LatNatDataset(valid_df, tokenizer)\n",
    "test_dataset = LatNatDataset(test_df, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=valid_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = torch.nn.RNN(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, seq, seq_len):\n",
    "        embedding = self.embedding(input=seq)\n",
    "        print(\"embedded\")\n",
    "        packed = pack_padded_sequence(input=embedding, lengths=seq_len, batch_first=True, enforce_sorted=True)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = torch.nn.RNN(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, seq, seq_len):\n",
    "        embedding = self.embedding(input=seq)\n",
    "        packed = pack_padded_sequence(input=embedding, lengths=seq_len, batch_first=True, enforce_sorted=True)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(tokenizer.latin_vocab_size, 128)\n",
    "decoder = Decoder(tokenizer.nat_vocab_size, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[189]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m _, hidden = encoder(x, x_len)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y.shape[\u001b[32m1\u001b[39m]):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     output, hidden = \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(output.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/conda/miniconda3/envs/dla3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/conda/miniconda3/envs/dla3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[185]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, seq, seq_len)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq, seq_len):\n\u001b[32m      9\u001b[39m     embedding = \u001b[38;5;28mself\u001b[39m.embedding(\u001b[38;5;28minput\u001b[39m=seq)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     packed = \u001b[43mpack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_sorted\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     output, hidden = \u001b[38;5;28mself\u001b[39m.rnn(packed)\n\u001b[32m     12\u001b[39m     output, _ = pad_packed_sequence(output, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/conda/miniconda3/envs/dla3/lib/python3.11/site-packages/torch/nn/utils/rnn.py:323\u001b[39m, in \u001b[36mpack_padded_sequence\u001b[39m\u001b[34m(input, lengths, batch_first, enforce_sorted)\u001b[39m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._C._get_tracing_state():\n\u001b[32m    316\u001b[39m         warnings.warn(\n\u001b[32m    317\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpack_padded_sequence has been called with a Python list of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    318\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msequence lengths. The tracer cannot track the data flow of Python \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    321\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    322\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     lengths = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    325\u001b[39m     lengths = lengths.to(dtype=torch.int64)\n",
      "\u001b[31mTypeError\u001b[39m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    x, x_len, y, y_len = batch\n",
    "    _, hidden = encoder(x, x_len)\n",
    "    for i in range(y.shape[1]):\n",
    "        output, hidden = decoder(y[:, i].unsqueeze(1), y_len)\n",
    "        print(output.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  82, 148, 161,  27,  15, 133, 120,  69,  58,  86,   1],\n",
       "        [  0,  32, 183, 174, 184,  27,  17, 147,  28,  15, 207,   1],\n",
       "        [  0,   6, 119, 174, 147, 142, 148,  82, 225,   1,   2,   2],\n",
       "        [  0,  10, 212,  84, 149, 163, 106,  83, 171,   1,   2,   2],\n",
       "        [  0,  83,  27,  19,  82, 186, 195,   1,   2,   2,   2,   2],\n",
       "        [  0, 124,  93,  82,  15, 135,  15,   1,   2,   2,   2,   2],\n",
       "        [  0, 224,  69,  66, 148,  62, 146,   1,   2,   2,   2,   2],\n",
       "        [  0,  35, 215,  96, 192,  15, 207,   1,   2,   2,   2,   2],\n",
       "        [  0,   4, 180, 147, 133, 120,  69,  62,   1,   2,   2,   2],\n",
       "        [  0,  95, 119,  16, 149, 159, 146,   1,   2,   2,   2,   2],\n",
       "        [  0, 135, 176, 121,  62,  15, 207,   1,   2,   2,   2,   2],\n",
       "        [  0, 216, 159, 224,  81,  62, 146,   1,   2,   2,   2,   2],\n",
       "        [  0,  83, 107, 135,  15,   1,   2,   2,   2,   2,   2,   2],\n",
       "        [  0,  82, 196,  93,  90,   1,   2,   2,   2,   2,   2,   2],\n",
       "        [  0,  96, 136,  93,  82, 146,   1,   2,   2,   2,   2,   2],\n",
       "        [  0, 135,  94,  15, 159, 133,   1,   2,   2,   2,   2,   2],\n",
       "        [  0,   5,  58, 133, 120,  62, 146,   1,   2,   2,   2,   2],\n",
       "        [  0,  25, 184,  93,  84, 195,   1,   2,   2,   2,   2,   2],\n",
       "        [  0,  39,  69,  62,  97, 171,   1,   2,   2,   2,   2,   2],\n",
       "        [  0, 120,  32, 186,  27,  16,   1,   2,   2,   2,   2,   2],\n",
       "        [  0,  89, 225, 155,   1,   2,   2,   2,   2,   2,   2,   2],\n",
       "        [  0,   9, 165,  28,  19, 146,   1,   2,   2,   2,   2,   2],\n",
       "        [  0,  19, 212, 219, 195,   1,   2,   2,   2,   2,   2,   2],\n",
       "        [  0,   4, 141,  30, 119,   1,   2,   2,   2,   2,   2,   2],\n",
       "        [  0,  10, 119,  17, 173, 171,   1,   2,   2,   2,   2,   2],\n",
       "        [  0,  29,  86, 133, 124,   1,   2,   2,   2,   2,   2,   2],\n",
       "        [  0, 169, 186, 119,   1,   2,   2,   2,   2,   2,   2,   2],\n",
       "        [  0,   4, 210, 133, 124,   1,   2,   2,   2,   2,   2,   2],\n",
       "        [  0,  48, 192,   1,   2,   2,   2,   2,   2,   2,   2,   2],\n",
       "        [  0,  37, 219, 147, 171,   1,   2,   2,   2,   2,   2,   2],\n",
       "        [  0, 243, 106,  84,   1,   2,   2,   2,   2,   2,   2,   2],\n",
       "        [  0,   6, 159, 149, 195,   1,   2,   2,   2,   2,   2,   2]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
